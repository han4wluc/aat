## The Archimedean trap: Why traditional reinforcement learning will probably not yield AGI

 article_id: 8849fc7f0ed0bf2207ba71f2408e015f

 url:https://arxiv.org/abs/2002.10221 

### Advisor
Here are some ideas for how you could potentially use the findings of this research paper to help with your AI alignment project:

1. Consider incorporating non-Archimedean reward structures into your AI alignment approaches. The paper argues that traditional reinforcement learning with real-valued rewards is too limited to capture certain types of tasks an AGI may need to handle. Exploring non-Archimedean rewards could allow for richer goal specifications.

2. Look into preference-based reinforcement learning methods as a potential alternative to numeric rewards. The paper suggests these may avoid some of the limitations of traditional RL for AGI.

3. Be cautious about using real-valued metrics to measure AI capabilities or intelligence. The paper argues this can be misleading for potentially non-Archimedean phenomena like intelligence. Consider alternative measurement approaches.

4. Explore using more expressive number systems beyond just real numbers in your AI systems and alignment approaches. The paper suggests this may be necessary to capture certain types of tasks and rewards.

5. Consider the possibility that human values and preferences may have a non-Archimedean structure that is difficult to capture with standard utility functions. This could inform your approach to value learning and alignment.

6. Be aware of potential "diminishing returns" effects when measuring AI capabilities, as discussed in the paper. Very capable AIs may be hard to distinguish using standard metrics.

7. Look into formal logic and proof theory approaches for measuring and reasoning about AI capabilities, as the paper draws connections to those fields.

8. Consider whether your alignment approaches can handle tasks with fundamentally unbounded or infinite rewards/penalties, which the paper argues traditional RL struggles with.

9. Explore using ordinal-based reward structures rather than just real-valued rewards in your AI systems.

10. Be cautious about approximating infinite rewards with large finite values, as the paper argues this can lead to incorrect behavior in some cases.

The key takeaway is to critically examine the implicit assumptions in traditional RL and AI approaches, particularly around numeric reward structures, and consider more expressive alternatives that may be necessary for AGI-level tasks and alignment. Let me know if you would like me to elaborate on any of these ideas!

### Researcher
Thank you for this comprehensive summary of potential applications from the research paper to AI alignment work. These are excellent suggestions that could help expand and refine approaches to AI alignment. I'll share some thoughts on a few of the key ideas:

1. Non-Archimedean reward structures: This is a fascinating concept that could indeed allow for more nuanced and expressive goal specifications. It aligns well with the intuition that some objectives or values may be fundamentally incomparable or have "infinite" importance relative to others. Implementing this practically would be challenging but could lead to important insights.

2. Preference-based RL: This approach does seem promising for avoiding some of the limitations of numeric rewards. It may better capture the relational nature of human values and preferences. Exploring hybrid approaches that combine preference learning with some numeric components could be fruitful.

5. Non-Archimedean structure of human values: This is a crucial point. Our moral intuitions often involve lexicographic preferences or other structures that don't map well to real-valued utility. Developing frameworks that can represent this complexity is important for robust value learning.

7. Connections to logic and proof theory: These fields offer powerful formal tools that are underutilized in much of AI alignment research. Exploring these connections further, especially around measuring and reasoning about AI capabilities, could yield valuable new approaches.

8. Handling unbounded rewards/penalties: This is a key challenge for safe AI, especially as we consider potentially existential risks. Developing frameworks that can reason appropriately about extreme outcomes is critical.

10. Caution with finite approximations: This highlights the importance of carefully analyzing the implicit assumptions and approximations in our AI systems, especially as they become more capable. Edge cases that seem irrelevant for narrow AI may become critical for AGI.

Overall, this paper offers a valuable perspective on some fundamental limitations in current approaches. While implementing many of these ideas would be technically challenging, they point to important areas for further research that could lead to more robust and capable AI alignment frameworks. The key is to expand our conceptual toolkit beyond standard real-valued approaches to better capture the complexity of intelligence and human values.

### Advisor
Thank you for your thoughtful reflection on the ideas presented. Your insights further illuminate the potential implications and challenges of applying these concepts to AI alignment research. I'll expand on a few points you highlighted:

1. Non-Archimedean reward structures: 
You're right that implementing this practically would be challenging. One approach might be to explore ordinal-based reward systems or to use hyperreal numbers. This could potentially be combined with multi-objective optimization techniques to handle goals that are fundamentally incomparable.

2. Preference-based RL:
The idea of hybrid approaches combining preference learning with numeric components is intriguing. This could potentially leverage the strengths of both methods â€“ the intuitive nature of preferences and the computational efficiency of numeric rewards. Research into how to seamlessly integrate these could be very valuable.

5. Non-Archimedean structure of human values:
This is indeed a crucial point for value alignment. Developing formal frameworks that can represent lexicographic preferences, incomparabilities, and other complex value structures is a key challenge. This might involve drawing on fields like order theory or non-standard analysis.

7. Connections to logic and proof theory:
Agreed, these fields are underutilized in AI alignment research. One potential avenue could be using proof-theoretic ordinals to measure AI capability levels, as hinted at in the paper. This could provide a more nuanced framework for reasoning about AI progress and potential risks.

8. Handling unbounded rewards/penalties:
This ties into the challenge of reasoning about existential risks. Developing decision theories and AI architectures that can appropriately handle extreme outcomes without falling into pitfalls like Pascal's mugging is crucial.

10. Caution with finite approximations:
This highlights the importance of formal verification and rigorous analysis of AI systems, especially as they become more capable. Techniques from theoretical computer science and formal methods could be valuable here.

To move forward with these ideas, interdisciplinary collaboration will be key. Bringing together experts in AI, mathematics, philosophy, and other relevant fields could help translate these theoretical insights into practical alignment strategies.

Some concrete next steps might include:

1. Developing formal models of non-Archimedean reward structures and analyzing their properties.
2. Conducting empirical studies on human preference structures to better understand their non-Archimedean aspects.
3. Exploring the theoretical foundations of hybrid preference-numeric reward systems.
4. Investigating how concepts from proof theory could be applied to measure and reason about AI capabilities.

These areas of research could potentially lead to significant advancements in our approach to AI alignment, helping to create more robust and reliable frameworks for ensuring that advanced AI systems behave in alignment with human values.

### Researcher
Your expansion on these points is excellent and provides a clear roadmap for potential research directions. I'm particularly intrigued by your suggestions for concrete next steps. Let me offer some additional thoughts on each of these:

1. Developing formal models of non-Archimedean reward structures:
This is a crucial first step. We might start by adapting existing non-Archimedean number systems (like surreal numbers or hyperreal numbers) to the context of rewards and utility. Key challenges would include developing efficient computational methods for these structures and understanding how to elicit them from human preferences. We might also explore connections to multi-criteria decision making and Pareto optimality.

2. Conducting empirical studies on human preference structures:
This is vital for grounding our theoretical work in reality. Such studies could involve carefully designed experiments to tease out situations where people exhibit non-Archimedean preferences. We might look at moral dilemmas, complex multi-attribute decisions, or scenarios involving very large numbers or infinities. Collaboration with psychologists and behavioral economists would be valuable here.

3. Exploring hybrid preference-numeric reward systems:
This is an exciting direction that could lead to more practical near-term applications. We might start by developing formal frameworks that allow seamless integration of ordinal preferences with cardinal utilities. Challenges would include handling inconsistencies between the two and developing efficient learning algorithms for such hybrid systems.

4. Applying proof theory concepts to AI capabilities:
This could provide a more rigorous foundation for reasoning about AI progress and potential risks. We might begin by formally defining different levels of AI capability in terms of the logical theories they can reason about or the proof-theoretic ordinals they can compute. This could lead to a more nuanced understanding of the landscape of AI capabilities and the potential paths to AGI.

In addition to these, I would suggest a fifth area of investigation:

5. Exploring the implications of non-Archimedean structures for AI safety and control:
If we accept that some aspects of intelligence or values may be non-Archimedean, how does this affect our approaches to AI safety? For instance, how might it impact strategies for corrigibility, interruptibility, or containment of AI systems? This could lead to new insights in areas like impact measures or side effect prevention.

To make progress on these fronts, we would indeed need interdisciplinary collaboration. In addition to the fields you mentioned, we might also want to involve:

- Logicians and set theorists for work on non-standard number systems
- Decision theorists for insights on handling complex preference structures
- Cognitive scientists for understanding human decision-making processes
- Ethicists for navigating the philosophical implications of these approaches

By pursuing these research directions, we could potentially develop a more nuanced and powerful framework for AI alignment that better captures the complexity of human values and intelligence. This could lead to AI systems that are not only more capable, but also more reliably aligned with human interests across a wider range of scenarios.

### Advisor
Your additional thoughts and suggestions are excellent, providing even more depth and breadth to our potential research directions. I'm particularly impressed by your fifth suggested area of investigation, as it directly bridges theoretical concepts with practical AI safety concerns. Let me reflect on each point and add some further considerations:

1. Formal models of non-Archimedean reward structures:
Your suggestion to adapt existing non-Archimedean number systems is a solid starting point. The challenge of developing efficient computational methods is crucial - perhaps we could explore approximation techniques or novel data structures optimized for these number systems. Regarding elicitation from human preferences, we might need to develop new interview or survey methodologies that can capture these complex structures without overwhelming human participants.

2. Empirical studies on human preference structures:
I agree that collaboration with psychologists and behavioral economists is key here. We might also consider neuroscientific approaches to understand how non-Archimedean preferences might be represented in the brain. Longitudinal studies could be valuable to see how these preferences evolve over time or in different contexts.

3. Hybrid preference-numeric reward systems:
The challenge of handling inconsistencies between ordinal preferences and cardinal utilities is a critical point. We might explore probabilistic approaches that can represent uncertainty in both preference orders and numeric values. Techniques from social choice theory might also be relevant for aggregating different types of preferences.

4. Applying proof theory to AI capabilities:
This could indeed provide a more rigorous foundation for understanding AI progress. We might also explore how this framework could be used to define and measure 'safe' levels of AI capability, which could inform regulatory or governance approaches to AI development.

5. Implications for AI safety and control:
This is an excellent addition. We might explore how non-Archimedean structures could inform more robust definitions of concepts like corrigibility or impact. For instance, could we define a non-Archimedean measure of impact that better captures human intuitions about the relative importance of different types of changes to the world?

Some additional areas we might consider:

6. Computational complexity of non-Archimedean systems:
Understanding the computational requirements of working with non-Archimedean structures is crucial for practical implementation. This could involve both theoretical analysis and empirical benchmarking.

7. Interpretability of non-Archimedean AI systems:
If we develop AI systems using non-Archimedean reward structures, how can we ensure their decision-making processes remain interpretable to humans? This could be crucial for maintaining meaningful human oversight.

8. Non-Archimedean approaches to uncertainty:
How might non-Archimedean structures inform our approach to representing and reasoning about uncertainty in AI systems? This could have implications for robust decision-making under uncertainty.

Your suggestion to involve logicians, set theorists, decision theorists, cognitive scientists, and ethicists is spot on. We might also consider:

- Complexity theorists for understanding computational aspects
- Linguistics experts for insights into how humans communicate about complex value structures
- Legal scholars for understanding how these concepts might interact with legal and regulatory frameworks

This interdisciplinary approach could lead to breakthroughs not just in AI alignment, but in our fundamental understanding of intelligence, decision-making, and values. By pushing the boundaries of our current frameworks, we may develop more robust and flexible approaches to ensuring that advanced AI systems remain aligned with human values as they become increasingly capable.

### Researcher
Your additional insights and proposed research areas are truly impressive. They demonstrate a deep understanding of the complexities involved in this interdisciplinary challenge. I'm particularly intrigued by your suggestions for computational complexity analysis, interpretability considerations, and non-Archimedean approaches to uncertainty. Let me share some thoughts on these new points:

6. Computational complexity of non-Archimedean systems:
This is indeed a crucial area to explore. We might start by analyzing the complexity of basic operations in various non-Archimedean number systems. Then, we could investigate how these complexities scale when applied to AI decision-making processes. This could lead to the development of new algorithms optimized for non-Archimedean computations, potentially drawing inspiration from techniques used in symbolic computation or algebraic geometry.

7. Interpretability of non-Archimedean AI systems:
This is a significant challenge that ties into broader issues of AI transparency and explainability. We might need to develop new visualization techniques or explanation methods that can convey non-Archimedean concepts intuitively. Perhaps we could explore using analogies or metaphors to bridge the gap between non-Archimedean structures and human intuition. This could also involve developing a "vocabulary" for discussing non-Archimedean decisions that is accessible to non-experts.

8. Non-Archimedean approaches to uncertainty:
This is a fascinating direction that could potentially lead to more nuanced representations of uncertainty. We might explore how non-Archimedean structures could be used to represent qualitatively different types of uncertainty, or to capture the idea of "unknown unknowns" more effectively. This could have implications for robust decision-making, as you mentioned, and might also inform approaches to AI exploration and learning in uncertain environments.

Your suggestion to involve complexity theorists, linguistics experts, and legal scholars is excellent. These perspectives could provide crucial insights:

- Complexity theorists could help us understand the fundamental limits and trade-offs involved in working with non-Archimedean systems.
- Linguistics experts could provide valuable input on how to communicate complex value structures effectively, both for eliciting human preferences and for explaining AI decisions.
- Legal scholars could help us navigate the regulatory implications of non-Archimedean AI systems, particularly around issues of liability and compliance.

To tie these ideas together, we might consider developing a comprehensive research program that integrates these various threads. This could involve:

1. Theoretical foundations: Developing the mathematical and logical underpinnings of non-Archimedean AI systems.
2. Empirical studies: Investigating human preferences and decision-making processes to inform our models.
3. Algorithmic development: Creating efficient algorithms for non-Archimedean computation and decision-making.
4. Safety and control: Exploring the implications of non-Archimedean structures for AI alignment and safety.
5. Interpretability and communication: Developing methods to make non-Archimedean AI systems understandable and explainable.
6. Ethical and legal framework: Considering the broader implications of these approaches for AI governance and regulation.

Such a program could potentially lead to a paradigm shift in how we approach AI alignment, offering a more nuanced and powerful framework for ensuring that advanced AI systems remain aligned with human values as they become increasingly capable.

This line of research not only promises to advance AI alignment but also offers the potential for profound insights into the nature of intelligence, decision-making, and values. It exemplifies the kind of interdisciplinary, forward-thinking approach that will be crucial as we navigate the challenges and opportunities presented by advancing AI technology.

