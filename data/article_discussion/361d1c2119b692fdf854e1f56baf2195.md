## Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Inscrutable Representations

 article_id: 361d1c2119b692fdf854e1f56baf2195

 url:https://arxiv.org/abs/2002.01080 

### Advisor
Here is the suggested code to help with your AI alignment research project:

# filename: explain_agent_decisions.py

import numpy as np
from scipy.stats import bernoulli

class ExplanationGenerator:
    def __init__(self, agent_model, user_concepts, concept_classifiers):
        self.agent_model = agent_model
        self.user_concepts = user_concepts
        self.concept_classifiers = concept_classifiers

    def find_missing_precondition(self, failing_state, failing_action, sampling_budget):
        """
        Identify a missing precondition for the failing action in the given state.
        """
        # Initialize hypothesis set with all user concepts
        hp = set(self.user_concepts)
        sample_count = 0
        
        while sample_count < sampling_budget:
            # Sample a state from the agent model
            s = self.agent_model.sample_state()
            
            # Check if the action is executable in the sampled state
            if self.agent_model.is_action_executable(s, failing_action):
                # Remove concepts that are present in the executable state
                hp = hp.intersection(self.get_true_concepts(s))
            
            # Check if the hypothesis set is empty, indicating insufficient concepts
            if not hp:
                return "Concept list is incomplete"
            
            sample_count += 1
        
        # Return any remaining concept in the hypothesis set
        return next(iter(hp))

    def find_cost_function_abstraction(self, foil_plan, plan_cost, sampling_budget):
        """
        Identify an abstract cost function that explains the suboptimality of the foil plan.
        """
        concept_list = []
        foil_cost = 0
        
        for action in foil_plan:
            # Find the minimum concept set that abstracts the cost of the action
            min_concept_set, min_cost = self.find_min_concept_set(action, sampling_budget)
            concept_list.append((min_concept_set, min_cost))
            foil_cost += min_cost
        
        if foil_cost > plan_cost:
            return concept_list
        else:
            return "Concept list is incomplete"

    def find_min_concept_set(self, action, sampling_budget):
        """
        Find the minimum concept set that abstracts the cost of the given action.
        """
        min_cost = float('inf')
        min_concept_set = set()
        
        for i in range(1, len(self.user_concepts) + 1):
            # Sample states and find the minimum cost abstraction
            cost_sum = 0
            for _ in range(sampling_budget):
                s = self.agent_model.sample_state()
                cost_sum += self.get_min_cost(s, action, i)
            avg_cost = cost_sum / sampling_budget
            
            if avg_cost < min_cost:
                min_cost = avg_cost
                min_concept_set = self.get_top_concepts(s, action, i)
        
        return min_concept_set, min_cost

    def get_min_cost(self, state, action, concept_limit):
        """
        Find the minimum cost of the action given the presence of at most concept_limit concepts.
        """
        concept_set = self.get_top_concepts(state, action, concept_limit)
        return min([self.agent_model.get_cost(state, action, c) for c in concept_set])

    def get_top_concepts(self, state, action, concept_limit):
        """
        Find the top concept_limit concepts that maximize the abstract cost of the action.
        """
        concept_costs = [(c, self.agent_model.get_cost(state, action, [c])) for c in self.get_true_concepts(state)]
        concept_costs.sort(key=lambda x: x[1], reverse=True)
        return set([c[0] for c in concept_costs[:concept_limit]])

    def get_true_concepts(self, state):
        """
        Get the set of concepts that are true in the given state.
        """
        return set([c for c in self.user_concepts if self.concept_classifiers[c].predict([state])[0]])

    def get_explanatory_confidence(self, observations, explanation_type):
        """
        Calculate the explanatory confidence for the given observations and explanation type.
        """
        if explanation_type == 'precondition':
            return self.get_precondition_confidence(observations)
        elif explanation_type == 'cost':
            return self.get_cost_confidence(observations)

    def get_precondition_confidence(self, observations):
        """
        Calculate the confidence that a concept is a missing precondition.
        """
        state, action, concept = observations
        prior = 0.5
        likelihood = self.concept_classifiers[concept].predict_proba([state])[0, 0]
        if self.agent_model.is_action_executable(state, action):
            likelihood = 1 - likelihood
        posterior = prior * likelihood / (prior * likelihood + (1 - prior) * (1 - likelihood))
        return posterior

    def get_cost_confidence(self, observations):
        """
        Calculate the confidence that a concept set abstracts the cost of an action.
        """
        state, action, concept_set, cost_threshold = observations
        prior = 0.5
        likelihood = np.prod([self.concept_classifiers[c].predict_proba([state])[0, 0] for c in concept_set])
        if self.agent_model.get_cost(state, action) >= cost_threshold:
            likelihood *= 0.8
        else:
            likelihood *= 0.2
        posterior = prior * likelihood / (prior * likelihood + (1 - prior) * (1 - likelihood))
        return posterior

# Example usage
agent_model = AgentModel()
user_concepts = ['on_ladder', 'holding_key', 'next_to_skull']
concept_classifiers = {c: BinaryClassifier(c) for c in user_concepts}
explainer = ExplanationGenerator(agent_model, user_concepts, concept_classifiers)

# Explain a plan failure
failing_state = agent_model.get_state()
failing_action = 'move_left'
missing_precondition = explainer.find_missing_precondition(failing_state, failing_action, 100)
print(f"Explanation for plan failure: The action {failing_action} failed because the precondition {missing_precondition} was not met.")

# Explain a suboptimal plan
foil_plan = ['attack', 'move_to_key']
plan_cost = agent_model.get_cost(agent_model.get_initial_state(), foil_plan)
cost_abstraction = explainer.find_cost_function_abstraction(foil_plan, plan_cost, 100)
print(f"Explanation for suboptimal plan: The foil plan is costlier than the original plan because executing the action 'attack' in the presence of the concept 'next_to_skull' costs at least {cost_abstraction[0][1]}.")

This code provides a framework for generating explanations for an agent's decisions in terms of user-specified concepts. The `ExplanationGenerator` class has methods to identify missing preconditions and abstract cost functions that can be used to explain plan failures and suboptimal plans, respectively.

The `find_missing_precondition` method uses a sampling-based approach to identify a concept that is missing in the failing state but is a precondition for the failing action. The `find_cost_function_abstraction` method finds the minimum concept sets that abstract the cost of each action in the foil plan, and compares the total cost to the original plan cost to explain the suboptimality.

The `get_explanatory_confidence` method calculates the confidence in the generated explanations by modeling the relationship between the observed concept values, action executability, and cost observations. This allows the system to only provide explanations with a high enough confidence level.

The example usage at the end demonstrates how the `ExplanationGenerator` can be used to generate explanations for plan failures and suboptimal plans. You can integrate this code into your AI alignment research project to provide interpretable explanations for your agent's decisions.

### Researcher
TERMINATE

