{"task": "relevancy", "last_date_published": "2020-01-28T00:00:00Z", "new_last_date_pulished": "2020-02-13T00:00:00Z", "articles": [{"id": "7010710588ce3649be49076da7782fd4", "title": "A high-precision abundance analysis of the nuclear benchmark star HD 20", "date_published": "2020-01-29T00:00:00Z", "relevancy": 7, "relevancy_justification": "The research paper discusses the chemical abundances in metal-poor stars, which is related to the origin and evolution of galaxies. However, it does not directly address AI alignment, so the relevancy is not extremely high."}, {"id": "4fdc8af399e0d3daceda3c5dbab7e577", "title": "Preventing Imitation Learning with Adversarial Policy Ensembles", "date_published": "2020-01-31T00:00:00Z", "relevancy": 8, "relevancy_justification": "The research paper presents a method to prevent imitation learning by training adversarial policy ensembles, which is directly relevant to my project on finding novel solutions to solve AI alignment. The paper introduces the concept of adversarial experts, which is a novel approach to AI alignment that I have not considered before. Additionally, the paper presents a mathematical justification for the notion of adversarial experts and provides experimental results demonstrating the effectiveness of the proposed method. This makes the paper highly relevant to my project."}, {"id": "361d1c2119b692fdf854e1f56baf2195", "title": "Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Inscrutable Representations", "date_published": "2020-02-04T00:00:00Z", "relevancy": 7, "relevancy_justification": "The research paper focuses on providing post-hoc symbolic explanations for sequential decision-making problems with inscrutable representations, which is directly relevant to AI alignment research. The paper introduces a framework that learns local symbolic dynamical models to explain the agent's decisions in terms of user-specified vocabulary terms. This aligns with the goal of finding novel solutions to solve AI alignment. However, the paper does not directly address the core challenges of AI alignment, such as value learning or inner alignment, which is why the relevancy score is not 10."}, {"id": "4f82da4121e763abf8f4862c2ab2a8b2", "title": "Student/Teacher Advising through Reward Augmentation", "date_published": "2020-02-07T00:00:00Z", "relevancy": 7, "relevancy_justification": "The research paper, while not directly about AI alignment, discusses methods and results related to understanding and predicting human values, which is a key aspect of AI alignment. The paper's focus on human values and their influence on decision-making makes it relevant to your project, although it doesn't provide novel solutions for AI alignment."}, {"id": "94c39b5469e869810a0ce1df4725c8af", "title": "Leveraging Rationales to Improve Human Task Performance", "date_published": "2020-02-11T00:00:00Z", "relevancy": null, "relevancy_justification": null}, {"id": "0914444ec4d305a581c85e254ea1225b", "title": "A Bounded Measure for Estimating the Benefit of Visualization", "date_published": "2020-02-12T00:00:00Z", "relevancy": 7, "relevancy_justification": "The research paper discusses the use of information theory in visualization, which is relevant to my project on AI alignment. However, it is not directly related to my specific research questions or methods. The paper also focuses on visualization, which is a specific application of information theory, rather than the broader aspects of information theory that I am interested in."}, {"id": "085c7909f407e916cec67c09112d9d3d", "title": "AI safety: state of the field through quantitative lens", "date_published": "2020-02-12T00:00:00Z", "relevancy": 8, "relevancy_justification": "The paper provides a comprehensive overview of the AI safety field, including technical AI safety, AI ethics, and AI policy, which are all relevant to your project on finding novel solutions to AI alignment. The paper also discusses the lack of quantitative insights into the field, which your project aims to address. Additionally, the paper highlights the importance of value alignment as a long-term research direction, which is a key aspect of your project. The only reason the relevancy is not 10 is because the paper is not focused specifically on AI alignment, but rather provides a broader overview of the AI safety field."}, {"id": "79c7fe8d20495ec5f5860550c56b661b", "title": "The Conditional Entropy Bottleneck", "date_published": "2020-02-13T00:00:00Z", "relevancy": 7, "relevancy_justification": "The research paper discusses the challenges in classical generalization and introduces a new objective function called the Conditional Entropy Bottleneck (CEB) that directly optimizes the Minimum Necessary Information (MNI) criterion. This is relevant to my project as it aims to find novel solutions to solve AI alignment, which involves understanding and optimizing the information flow in AI systems. The paper also discusses the issues of vulnerability to adversarial examples, poor out-of-distribution detection, miscalibrated predictions, and overfitting to training data, all of which are relevant to the AI alignment problem. Additionally, the paper compares the CEB objective with the Information Bottleneck (IB) objective, which is a related concept in AI alignment research."}, {"id": "fa2a0311913b34e3d5b91dcef29ca7ba", "title": "CEB Improves Model Robustness", "date_published": "2020-02-13T00:00:00Z", "relevancy": 7, "relevancy_justification": "The research paper discusses techniques to improve model robustness, which is a key aspect of AI alignment. The paper presents an empirical study on the use of the Conditional Entropy Bottleneck (CEB) to improve model robustness in various scenarios, including test set generalization, worst-case robustness, and typical-case robustness. The paper also compares the results on L2 and L\u221e PGD attacks from Madry et al. (2017) to their own results on the same baseline architecture. The paper's focus on improving model robustness and its empirical approach make it directly relevant to my project on finding novel solutions to solve AI alignment."}, {"id": "e94437477a3c819b55faade7ca8d0112", "title": "A Simple Framework for Contrastive Learning of Visual Representations", "date_published": "2020-02-13T00:00:00Z", "relevancy": 7, "relevancy_justification": "The paper discusses contrastive learning, a technique relevant to AI alignment, as it aims to learn representations that are invariant to certain transformations. However, the paper focuses on visual representations and does not directly address the challenges of AI alignment, such as value learning or inner alignment. The paper also does not discuss the interpretability or robustness of the learned representations, which are important aspects of AI alignment. Therefore, while the paper is relevant, it is not directly applicable to the project of finding novel solutions to solve AI alignment."}]}